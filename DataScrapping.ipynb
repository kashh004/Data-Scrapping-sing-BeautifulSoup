{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ec95dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1acef281",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Saved 100 quotes to quotes.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "class QuoteScraper:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.quotes = []\n",
    "\n",
    "    def scrape_quotes(self):\n",
    "        \"\"\"Scrapes quotes from multiple pages of the website.\"\"\"\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            print(f\"Scraping page {page_number}...\")\n",
    "            response = requests.get(f\"{self.base_url}/page/{page_number}/\")\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all quotes on the page\n",
    "            quote_elements = soup.find_all('div', class_='quote')\n",
    "\n",
    "            # If no quotes are found, exit the loop\n",
    "            if not quote_elements:\n",
    "                break\n",
    "\n",
    "            for quote_elem in quote_elements:\n",
    "                quote_text = quote_elem.find('span', class_='text').get_text()\n",
    "                author = quote_elem.find('small', class_='author').get_text()\n",
    "                tags = [tag.get_text() for tag in quote_elem.find_all('a', class_='tag')]\n",
    "\n",
    "                self.quotes.append({\n",
    "                    'Quote': quote_text,\n",
    "                    'Author': author,\n",
    "                    'Tags': ', '.join(tags)\n",
    "                })\n",
    "\n",
    "            page_number += 1\n",
    "\n",
    "    def save_to_csv(self, filename):\n",
    "        \"\"\"Saves the scraped quotes to a CSV file.\"\"\"\n",
    "        df = pd.DataFrame(self.quotes)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved {len(self.quotes)} quotes to {filename}\")\n",
    "\n",
    "def main():\n",
    "    # Base URL of the quotes website\n",
    "    base_url = \"http://quotes.toscrape.com\"\n",
    "    \n",
    "    # Initialize the QuoteScraper\n",
    "    scraper = QuoteScraper(base_url)\n",
    "\n",
    "    # Scrape quotes from the website\n",
    "    scraper.scrape_quotes()\n",
    "\n",
    "    # Save the scraped quotes to a CSV file\n",
    "    scraper.save_to_csv(\"quotes.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98871acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
